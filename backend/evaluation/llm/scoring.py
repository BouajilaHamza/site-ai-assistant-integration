from rouge_score import rouge_scorer

def calculate_llm_metrics(generated_answer: str, reference_answer: str) -> dict:
    """
    Calculate LLM response metrics: ROUGE and BERTScore.
    Args:
        generated_answer (str): The answer generated by the LLM.
        reference_answer (str): The reference (ground truth) answer.
    Returns:
        dict: Dictionary containing ROUGE and BERTScore metrics.
    """
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    rouge_scores = rouge.score(reference_answer, generated_answer)
    # BERTScore: expects a list of dicts, so we wrap the result
    from backend.evaluation.utils import bert_score
    bert = bert_score(generated_answer, reference_answer)
    return {
        "rouge1": rouge_scores['rouge1'].fmeasure,
        "rougeL": rouge_scores['rougeL'].fmeasure,
        "bert_score": bert
    }
