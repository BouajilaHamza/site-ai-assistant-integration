from langchain_groq import ChatGroq
from backend.utils.validation import calculate_retrieval_metrics, calculate_llm_metrics
from langchain_groq import ChatGroq
from langchain_community.document_loaders.firecrawl import FireCrawlLoader
from .vector_store import VectorStore
from backend.core.config import settings
import requests
from bs4 import BeautifulSoup
import asyncio
from typing import List,Dict
groq_client = ChatGroq(model="llama-3.3-70b-versatile",api_key=settings.GROQ_API_KEY)
vector_store = VectorStore()

# New helper function to extract links from sitemap.xml
def extract_sitemap_links(base_url: str) -> list:
    sitemap_url = base_url.rstrip("/") + "/sitemap.xml"
    try:
        r = requests.get(sitemap_url)
        if r.status_code == 200:
            soup = BeautifulSoup(r.text,features="xml")    
            return [loc.text.replace("\r\n", "").strip() for loc in soup.find_all('loc')]
    except Exception as e:
        print(f"Error extracting sitemap: {e}")
    return []

async def initialize_knowledge_base():
    links = extract_sitemap_links(settings.BASE_URL)
    documents = []
    for link in links:
        await asyncio.sleep(25)  # non-blocking sleep
        try:
            loader = FireCrawlLoader(url=link, api_key=settings.FIRECRAWL_API_KEY, mode="scrape")
            docs = await asyncio.to_thread(loader.load)  # run loader.load() without blocking
            documents.extend(docs)
        except Exception as e:
            print(f"Error loading document: {e} {link}")
    vector_store.add_documents(documents)


groq_client = ChatGroq()

async def validate_rag_system(retrieved_docs: List[str], ground_truth_docs: List[str], 
                              generated_answer: str, reference_answer: str) -> Dict[str, Dict[str, float]]:
    """
    Validate the RAG system by calculating retrieval and LLM metrics.

    Args:
        retrieved_docs (List[str]): List of retrieved document URLs.
        ground_truth_docs (List[str]): List of ground truth document URLs.
        generated_answer (str): The answer generated by the LLM.
        reference_answer (str): The reference (ground truth) answer.

    Returns:
        Dict[str, Dict[str, float]]: Dictionary containing retrieval and LLM metrics.
    """
    retrieval_metrics = calculate_retrieval_metrics(retrieved_docs, ground_truth_docs)
    llm_metrics = calculate_llm_metrics(generated_answer, reference_answer)

    return {
        "retrieval_metrics": retrieval_metrics,
        "llm_metrics": llm_metrics
    }
