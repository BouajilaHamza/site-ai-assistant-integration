from backend.utils.validation import calculate_retrieval_metrics, calculate_llm_metrics
from backend.services.vector_store import VectorStore
from backend.core.config import settings
from backend.utils.lang_detect_utils import MODEL_PATH
from langchain_community.document_loaders.sitemap import SitemapLoader
from langchain.docstore.document import Document
from langchain_groq import ChatGroq
from bs4 import BeautifulSoup
from typing import List, Dict
import requests
import fasttext



groq_client = ChatGroq(model="llama-3.3-70b-versatile",api_key=settings.GROQ_API_KEY)
# groq_client_allam = ChatGroq(model="allam-2-7b", api_key=settings.GROQ_API_KEY)
vector_store = VectorStore()

# Removed the immediate loading of the model at the module level
language_model = None

def load_language_model():
    global language_model
    if language_model is None:
        language_model = fasttext.load_model(str(MODEL_PATH))







# New helper function to extract links from sitemap.xml
def extract_sitemap_links(base_url: str) -> list:
    sitemap_url = base_url.rstrip("/") + "/sitemap.xml"
    try:
        r = requests.get(sitemap_url)
        if r.status_code == 200:
            soup = BeautifulSoup(r.text,features="xml")    
            return [loc.text.replace("\r\n", "").strip() for loc in soup.find_all('loc')]
    except Exception as e:
        print(f"Error extracting sitemap: {e}")
        return []

async def initialize_knowledge_base():
    sitemap_urls = extract_sitemap_links(settings.BASE_URL)
    docs = []
    for url in sitemap_urls[:1]:
        print(f"Processing sitemap: {url}")
        loader = SitemapLoader(web_path=url,)
        doc = loader.aload()
        docs.extend(doc)
    print(f"Total documents loaded: {len(docs)}")
    docs = [Document(page_content=doc.page_content, 
                        metadata={"url": doc.metadata["source"],
                                    "title": doc.metadata.get("title", "")}
                        ) 
                        for doc in docs]
    vector_store.add_documents(docs)


def detect_language(text: str) -> str:
    load_language_model()
    print(f"Detecting language for text: {text}")
    prediction = language_model.predict(text, k=1)  # Get the top prediction
    print(prediction)
    lang_code = prediction[0][0].replace("__label__", "")
    return lang_code


async def query_knowledge_base(question: str) -> str:
    relevant_docs = vector_store.similarity_search(question)
    context = "\n".join([doc.page_content for doc in relevant_docs])

    language = detect_language(question)
    print(language)
    if language in ["ar","fa"]:
        prompt = f"""بناءً على السياق التالي:
        {context}

        السؤال: {question}
        الإجابة:"""
        return await groq_client.ainvoke(prompt)
    else:
        prompt = f"""Based on the following context:
        {context}

        Question: {question}
        Answer:"""
        return await groq_client.ainvoke(prompt)


async def validate_rag_system(retrieved_docs: List[str], ground_truth_docs: List[str], 
                              generated_answer: str, reference_answer: str) -> Dict[str, Dict[str, float]]:
    """
    Validate the RAG system by calculating retrieval and LLM metrics.

    Args:
        retrieved_docs (List[str]): List of retrieved document URLs.
        ground_truth_docs (List[str]): List of ground truth document URLs.
        generated_answer (str): The answer generated by the LLM.
        reference_answer (str): The reference (ground truth) answer.

    Returns:
        Dict[str, Dict[str, float]]: Dictionary containing retrieval and LLM metrics.
    """
    retrieval_metrics = calculate_retrieval_metrics(retrieved_docs, ground_truth_docs)
    llm_metrics = calculate_llm_metrics(generated_answer, reference_answer)

    return {
        "retrieval_metrics": retrieval_metrics,
        "llm_metrics": llm_metrics
    }
