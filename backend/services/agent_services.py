from backend.utils.validation import calculate_retrieval_metrics, calculate_llm_metrics
from backend.core.config import settings
from backend.utils.lang_detect_utils import MODEL_PATH
from backend.services.vector_store import vector_store
from langchain_groq import ChatGroq
from typing import List, Dict
import fasttext



groq_client = ChatGroq(model="llama-3.3-70b-versatile",api_key=settings.GROQ_API_KEY)
# groq_client_allam = ChatGroq(model="allam-2-7b", api_key=settings.GROQ_API_KEY)

# Removed the immediate loading of the model at the module level
language_model = None

def load_language_model():
    global language_model
    if language_model is None:
        language_model = fasttext.load_model(str(MODEL_PATH))











def detect_language(text: str) -> str:
    load_language_model()
    print(f"Detecting language for text: {text}")
    prediction = language_model.predict(text, k=1)  # Get the top prediction
    print(prediction)
    lang_code = prediction[0][0].replace("__label__", "")
    return lang_code


async def query_knowledge_base(question: str) -> str:
    relevant_docs = vector_store.similarity_search(question)
    context = "\n".join([doc.page_content for doc in relevant_docs])

    language = detect_language(question)
    print(language)
    if language in ["ar","fa"]:
        prompt = f"""بناءً على السياق التالي:
        {context}

        السؤال: {question}
        الإجابة:"""
        return await groq_client.ainvoke(prompt)
    else:
        prompt = f"""Based on the following context:
        {context}

        Question: {question}
        Answer:"""
        return await groq_client.ainvoke(prompt)


async def validate_rag_system(retrieved_docs: List[str], ground_truth_docs: List[str], 
                              generated_answer: str, reference_answer: str) -> Dict[str, Dict[str, float]]:
    """
    Validate the RAG system by calculating retrieval and LLM metrics.

    Args:
        retrieved_docs (List[str]): List of retrieved document URLs.
        ground_truth_docs (List[str]): List of ground truth document URLs.
        generated_answer (str): The answer generated by the LLM.
        reference_answer (str): The reference (ground truth) answer.

    Returns:
        Dict[str, Dict[str, float]]: Dictionary containing retrieval and LLM metrics.
    """
    retrieval_metrics = calculate_retrieval_metrics(retrieved_docs, ground_truth_docs)
    llm_metrics = calculate_llm_metrics(generated_answer, reference_answer)

    return {
        "retrieval_metrics": retrieval_metrics,
        "llm_metrics": llm_metrics
    }
