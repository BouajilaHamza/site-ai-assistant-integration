from typing import List, Dict
from sklearn.metrics import precision_score, recall_score, f1_score
from rouge_score import rouge_scorer
from bert_score import score as bert_score



def calculate_retrieval_metrics(retrieved_docs: List[str], ground_truth_docs: List[str]) -> Dict[str, float]:
    """
    Calculate retrieval metrics: Precision, Recall, and F1-score.

    Args:
        retrieved_docs (List[str]): List of retrieved document URLs.
        ground_truth_docs (List[str]): List of ground truth document URLs.

    Returns:
        Dict[str, float]: Dictionary containing precision, recall, and F1-score.
    """
    # Convert to binary relevance (1 if in ground truth, 0 otherwise)
    y_true = [1 if doc in ground_truth_docs else 0 for doc in retrieved_docs]
    y_pred = [1] * len(retrieved_docs)  # All retrieved docs are considered relevant

    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    return {"precision": precision, "recall": recall, "f1_score": f1}

def calculate_llm_metrics(generated_answer: str, reference_answer: str) -> Dict[str, float]:
    """
    Calculate LLM response metrics: ROUGE and BERTScore.

    Args:
        generated_answer (str): The answer generated by the LLM.
        reference_answer (str): The reference (ground truth) answer.

    Returns:
        Dict[str, float]: Dictionary containing ROUGE and BERTScore metrics.
    """
    # ROUGE
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    rouge_scores = rouge.score(reference_answer, generated_answer)

    # BERTScore
    bert_p, bert_r, bert_f1 = bert_score([generated_answer], [reference_answer], lang="en")

    return {
        "rouge1": rouge_scores['rouge1'].fmeasure,
        "rougeL": rouge_scores['rougeL'].fmeasure,
        "bert_score": bert_f1.mean().item()
    }